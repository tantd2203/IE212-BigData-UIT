{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-09T05:52:31.083501Z",
     "start_time": "2024-05-09T05:52:28.553420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark\n",
      "  Using cached spark-0.2.1-py3-none-any.whl\n",
      "Installing collected packages: spark\n",
      "Successfully installed spark-0.2.1\n"
     ]
    }
   ],
   "source": [
    "! pip install spark\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Using cached kafka_python-2.0.2-py2.py3-none-any.whl.metadata (7.8 kB)\n",
      "Using cached kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T05:53:16.377771Z",
     "start_time": "2024-05-09T05:53:13.289085Z"
    }
   },
   "id": "f412213ba713a765",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in d:\\udemy\\airflowtutorial\\.venv\\ie212-bigdata-uit\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T05:53:18.404059Z",
     "start_time": "2024-05-09T05:53:16.378780Z"
    }
   },
   "id": "3f293339246473fe",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Pandas\n",
      "  Using cached pandas-2.2.2-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.23.2 (from Pandas)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\udemy\\airflowtutorial\\.venv\\ie212-bigdata-uit\\lib\\site-packages (from Pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from Pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from Pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\udemy\\airflowtutorial\\.venv\\ie212-bigdata-uit\\lib\\site-packages (from python-dateutil>=2.8.2->Pandas) (1.16.0)\n",
      "Using cached pandas-2.2.2-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, Pandas\n",
      "Successfully installed Pandas-2.2.2 numpy-1.26.4 pytz-2024.1 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "!pip install Pandas"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T05:54:23.548289Z",
     "start_time": "2024-05-09T05:54:06.998948Z"
    }
   },
   "id": "aa90278e38dc86eb",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x1a5ffa7cd50>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://DESKTOP-VIFGORN:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local</code></dd>\n              <dt>AppName</dt>\n                <dd><code>kafka-example</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "scala_version = '2.12'  # your scala version\n",
    "spark_version = '3.5.0'  # your spark version\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.6.0'  #your kafka version\n",
    "]\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"kafka-example\").config(\"spark.jars.packages\",\n",
    "                                                                             \",\".join(packages)).getOrCreate()\n",
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T06:16:48.027757Z",
     "start_time": "2024-05-09T06:16:48.013897Z"
    }
   },
   "id": "6c53d2ff4e97e71d",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Creating a Kafka Source for Batch Queries**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "560d71b81b0492de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create dataframe from Kafka data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a1787d7a25d5beb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "topic_name = 'RandomNumber'\n",
    "kafka_server = 'localhost:9092'\n",
    "kafkaDf = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafka_server).option(\"subscribe\",\n",
    "                                                                                            topic_name).option(\n",
    "    \"startingOffsets\",\n",
    "    \"earliest\").load()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T06:17:04.235094Z",
     "start_time": "2024-05-09T06:17:04.212985Z"
    }
   },
   "id": "25bee62ecd1ddb62",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show data (converting dataframe to pandas for cleaner view of data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5b89fa500cad8ba"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      key                                              value         topic  \\\n0    None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...  RandomNumber   \n1    None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...  RandomNumber   \n2    None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...  RandomNumber   \n3    None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...  RandomNumber   \n4    None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...  RandomNumber   \n..    ...                                                ...           ...   \n191  None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...  RandomNumber   \n192  None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...  RandomNumber   \n193  None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...  RandomNumber   \n194  None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...  RandomNumber   \n195  None  [123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...  RandomNumber   \n\n     partition  offset               timestamp  timestampType  \n0            0      92 2024-05-09 13:00:51.329              0  \n1            0      93 2024-05-09 13:00:56.332              0  \n2            0      94 2024-05-09 13:01:01.364              0  \n3            0      95 2024-05-09 13:01:06.374              0  \n4            0      96 2024-05-09 13:01:11.466              0  \n..         ...     ...                     ...            ...  \n191          0     283 2024-05-09 13:16:46.819              0  \n192          0     284 2024-05-09 13:16:51.820              0  \n193          0     285 2024-05-09 13:16:56.821              0  \n194          0     286 2024-05-09 13:17:01.822              0  \n195          0     287 2024-05-09 13:17:06.823              0  \n\n[196 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>value</th>\n      <th>topic</th>\n      <th>partition</th>\n      <th>offset</th>\n      <th>timestamp</th>\n      <th>timestampType</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>None</td>\n      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n      <td>RandomNumber</td>\n      <td>0</td>\n      <td>92</td>\n      <td>2024-05-09 13:00:51.329</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>None</td>\n      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n      <td>RandomNumber</td>\n      <td>0</td>\n      <td>93</td>\n      <td>2024-05-09 13:00:56.332</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>None</td>\n      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n      <td>RandomNumber</td>\n      <td>0</td>\n      <td>94</td>\n      <td>2024-05-09 13:01:01.364</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>None</td>\n      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n      <td>RandomNumber</td>\n      <td>0</td>\n      <td>95</td>\n      <td>2024-05-09 13:01:06.374</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>None</td>\n      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n      <td>RandomNumber</td>\n      <td>0</td>\n      <td>96</td>\n      <td>2024-05-09 13:01:11.466</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>None</td>\n      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n      <td>RandomNumber</td>\n      <td>0</td>\n      <td>283</td>\n      <td>2024-05-09 13:16:46.819</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>None</td>\n      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n      <td>RandomNumber</td>\n      <td>0</td>\n      <td>284</td>\n      <td>2024-05-09 13:16:51.820</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>None</td>\n      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n      <td>RandomNumber</td>\n      <td>0</td>\n      <td>285</td>\n      <td>2024-05-09 13:16:56.821</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>None</td>\n      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n      <td>RandomNumber</td>\n      <td>0</td>\n      <td>286</td>\n      <td>2024-05-09 13:17:01.822</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>None</td>\n      <td>[123, 34, 110, 117, 109, 98, 101, 114, 34, 58,...</td>\n      <td>RandomNumber</td>\n      <td>0</td>\n      <td>287</td>\n      <td>2024-05-09 13:17:06.823</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>196 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafkaDf.toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T06:17:08.951603Z",
     "start_time": "2024-05-09T06:17:08.181749Z"
    }
   },
   "id": "675d843cf6346e20",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show streaming data using for loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfaa169dfe9093d0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "batchDF = kafkaDf.select(col('topic'), col('offset'), col('value').cast('string').substr(12, 1).alias('rand_number'))\n",
    "from time import sleep\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "for x in range(0, 2000):\n",
    "    try:\n",
    "        print(\"Showing live view refreshed every 5 seconds\")\n",
    "        print(f\"Seconds passed: {x * 5}\")\n",
    "        display(batchDF.toPandas())\n",
    "        sleep(5)\n",
    "        clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"break\")\n",
    "    break\n",
    "print(\"Live view ended...\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-09T06:21:56.414362Z"
    }
   },
   "id": "91bc9f48ca20ebba",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    " Perform some data aggregation and show live results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2089a6edbeaa9fd3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batchCountDF = batchDF.groupBy('rand_number').count()\n",
    "for x in range(0, 2000):\n",
    "    try:\n",
    "        print(\"Showing live view refreshed every 5 seconds\")\n",
    "        print(f\"Seconds passed: {x * 5}\")\n",
    "        display(batchCountDF.toPandas())\n",
    "        sleep(100)\n",
    "        clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"break\")\n",
    "    break\n",
    "print(\"Live view ended...\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T06:21:56.412358Z",
     "start_time": "2024-05-09T06:21:56.412358Z"
    }
   },
   "id": "dde9c4ce11e781c3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating a Kafka Source for Streaming Queries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ad5955a69ca64eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Streaming dataframe from Kafka"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a60b522fc0f6333b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "streamRawDf = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafka_server).option(\"subscribe\",topic_name).load()\n",
    "streamDF = streamRawDf.select(col('topic'), col('offset'),col('value').cast('string').substr(12, 1).alias('rand_number'))\n",
    "checkEvenDF = streamDF.withColumn('Is_Even', col('rand_number').cast('int') % 2 == 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T06:19:25.003496Z",
     "start_time": "2024-05-09T06:19:24.962148Z"
    }
   },
   "id": "f64d0a6fdcf7689a",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();\nkafka",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m display(\u001B[43mcheckEvenDF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoPandas\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mD:\\UngDung\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:202\u001B[0m, in \u001B[0;36mPandasConversionMixin.toPandas\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    199\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m    201\u001B[0m \u001B[38;5;66;03m# Below is toPandas without Arrow optimization.\u001B[39;00m\n\u001B[1;32m--> 202\u001B[0m rows \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(rows) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    204\u001B[0m     pdf \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39mfrom_records(\n\u001B[0;32m    205\u001B[0m         rows, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(rows)), columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m    206\u001B[0m     )\n",
      "File \u001B[1;32mD:\\UngDung\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:1257\u001B[0m, in \u001B[0;36mDataFrame.collect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1237\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001B[39;00m\n\u001B[0;32m   1238\u001B[0m \n\u001B[0;32m   1239\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1254\u001B[0m \u001B[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001B[39;00m\n\u001B[0;32m   1255\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1256\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc):\n\u001B[1;32m-> 1257\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectToPython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1258\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001B[1;32mD:\\UngDung\\spark-3.5.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mD:\\UngDung\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: Queries with streaming sources must be executed with writeStream.start();\nkafka"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T06:21:56.411353Z",
     "start_time": "2024-05-09T06:21:55.920488Z"
    }
   },
   "id": "1d8d0420bbe98996",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write stream"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52360dfaf4e8f96c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "randNum = str(randint(0, 10000))\n",
    "q1name = \"queryNumber\" + randNum\n",
    "q2name = \"queryCheckEven\" + randNum\n",
    "stream_writer1 = (\n",
    "    streamDF.writeStream.queryName(q1name).trigger(processingTime=\"5 seconds\").outputMode(\"append\").format(\"memory\"))\n",
    "stream_writer2 = (\n",
    "    checkEvenDF.writeStream.queryName(q2name).trigger(processingTime=\"5 seconds\").outputMode(\"append\").format(\"memory\"))\n",
    "query1 = stream_writer1.start()\n",
    "query2 = stream_writer2.start()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T06:19:35.523341Z",
     "start_time": "2024-05-09T06:19:34.651106Z"
    }
   },
   "id": "92e265a3bfe2588c",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "View streaming result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "794c3d7b393896fb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing live view refreshed every 5 seconds\n",
      "Seconds passed : 95\n"
     ]
    },
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [topic, offset, rand_number]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>offset</th>\n      <th>rand_number</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [topic, offset, rand_number, Is_Even]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>offset</th>\n      <th>rand_number</th>\n      <th>Is_Even</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught Keyboard Interrupt\n",
      "Closing\n"
     ]
    }
   ],
   "source": [
    "for x in range(0, 2000):\n",
    "    try:\n",
    "        print(\"showing live view refreshed every 5 seconds\")\n",
    "        print(f\"Seconds passed : {x * 5}\")\n",
    "        result1 = spark.sql(f\"SELECT * from {query1.name}\")\n",
    "        result2 = spark.sql(f\"SELECT * from {query2.name}\")\n",
    "        display(result1.toPandas())\n",
    "        display(result2.toPandas())\n",
    "        sleep(5)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Caught Keyboard Interrupt\")\n",
    "        break\n",
    "print(\"Closing\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "856f056ba4d3eb64",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8d40945ef9a909be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
