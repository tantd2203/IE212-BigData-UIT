{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e8765c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-16T09:53:29.225572Z",
     "iopub.status.busy": "2024-04-16T09:53:29.224901Z",
     "iopub.status.idle": "2024-04-16T09:53:30.377745Z",
     "shell.execute_reply": "2024-04-16T09:53:30.376446Z"
    },
    "papermill": {
     "duration": 1.168518,
     "end_time": "2024-04-16T09:53:30.381211",
     "exception": false,
     "start_time": "2024-04-16T09:53:29.212693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/sparknpl/training.csv\n",
      "/kaggle/input/sparknpl/testdata.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ca073e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:53:30.404573Z",
     "iopub.status.busy": "2024-04-16T09:53:30.404012Z",
     "iopub.status.idle": "2024-04-16T09:54:29.403227Z",
     "shell.execute_reply": "2024-04-16T09:54:29.401319Z"
    },
    "papermill": {
     "duration": 59.014218,
     "end_time": "2024-04-16T09:54:29.405935",
     "exception": false,
     "start_time": "2024-04-16T09:53:30.391717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=871ad780f41988d60724bc3781b9f28a38374ac94d1f29b60da429322c972122\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: pyspark\r\n",
      "Successfully installed pyspark-3.5.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6452a6ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:54:29.454725Z",
     "iopub.status.busy": "2024-04-16T09:54:29.454255Z",
     "iopub.status.idle": "2024-04-16T09:54:47.502661Z",
     "shell.execute_reply": "2024-04-16T09:54:47.500796Z"
    },
    "papermill": {
     "duration": 18.076582,
     "end_time": "2024-04-16T09:54:47.505539",
     "exception": false,
     "start_time": "2024-04-16T09:54:29.428957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark\r\n",
      "  Downloading spark-0.2.1.tar.gz (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: spark\r\n",
      "  Building wheel for spark (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for spark: filename=spark-0.2.1-py3-none-any.whl size=58748 sha256=37d9741e61fd9f27549ad2e540099d61309fe79fd1262ffc626e072573e738ac\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/88/77/b4131110ea4094540f7b47c6d62a649807d7e94800da5eab0b\r\n",
      "Successfully built spark\r\n",
      "Installing collected packages: spark\r\n",
      "Successfully installed spark-0.2.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c9b278",
   "metadata": {
    "papermill": {
     "duration": 0.023976,
     "end_time": "2024-04-16T09:54:47.554982",
     "exception": false,
     "start_time": "2024-04-16T09:54:47.531006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Thêm thử viện và môn tả chi tiết thư viện đó làm gì ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5660a310",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:54:47.605326Z",
     "iopub.status.busy": "2024-04-16T09:54:47.604897Z",
     "iopub.status.idle": "2024-04-16T09:54:56.602256Z",
     "shell.execute_reply": "2024-04-16T09:54:56.599942Z"
    },
    "papermill": {
     "duration": 9.025234,
     "end_time": "2024-04-16T09:54:56.604469",
     "exception": true,
     "start_time": "2024-04-16T09:54:47.579235",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/16 09:54:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/04/16 09:54:55 ERROR SparkContext: Error initializing SparkContext.\n",
      "org.apache.spark.SparkIllegalArgumentException: [INVALID_EXECUTOR_MEMORY] Executor memory 10 must be at least 471859200. Please increase executor memory using the --executor-memory option or \"spark.executor.memory\" in Spark configuration.\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:230)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:201)\n",
      "\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:325)\n",
      "\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)\n",
      "\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:483)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkIllegalArgumentException: [INVALID_EXECUTOR_MEMORY] Executor memory 10 must be at least 471859200. Please increase executor memory using the --executor-memory option or \"spark.executor.memory\" in Spark configuration.\n\tat org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:230)\n\tat org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:201)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:325)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:483)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m f\n\u001b[1;32m      7\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      8\u001b[0m     \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModelTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m pd\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mmax_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m pd\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mmax_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkIllegalArgumentException: [INVALID_EXECUTOR_MEMORY] Executor memory 10 must be at least 471859200. Please increase executor memory using the --executor-memory option or \"spark.executor.memory\" in Spark configuration.\n\tat org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:230)\n\tat org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:201)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:325)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:483)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ModelTraining\")\n",
    "    .config(\"spark.executor.memory\", \"10\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 250\n",
    "pd.options.display.max_colwidth = 150\n",
    "sns.set(color_codes=True)\n",
    "#  tạo tên và lưu vô ram có 6g\n",
    "schema = \"polarity FLOAT, id LONG, date_time STRING, query STRING, user STRING, text STRING\"\n",
    "\n",
    "IN_PATH_TEST = \"/kaggle/input/sparknpl/testdata.csv\"\n",
    "IN_PATH_TRAIN = \"/kaggle/input/sparknpl/training.csv\"\n",
    "OUT_PATH_RAW = \"RAW\"\n",
    "\n",
    "# polarity là độ phổ biến\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a007c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:44:29.433854Z",
     "iopub.status.busy": "2024-04-16T09:44:29.433374Z",
     "iopub.status.idle": "2024-04-16T09:44:29.497270Z",
     "shell.execute_reply": "2024-04-16T09:44:29.496262Z",
     "shell.execute_reply.started": "2024-04-16T09:44:29.433819Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tạo ra format cho data frame\n",
    "spark_reader = spark.read.schema(schema)\n",
    "# \n",
    "test_data = spark_reader.csv(IN_PATH_TEST, header=False).cache()\n",
    "training_data = spark_reader.csv(IN_PATH_TRAIN, header=False).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855e7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:44:32.450785Z",
     "iopub.status.busy": "2024-04-16T09:44:32.450330Z",
     "iopub.status.idle": "2024-04-16T09:44:32.644073Z",
     "shell.execute_reply": "2024-04-16T09:44:32.642815Z",
     "shell.execute_reply.started": "2024-04-16T09:44:32.450751Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  show data test\n",
    "test_data.filter(f.col(\"polarity\")==4.0).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b8e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:44:35.362880Z",
     "iopub.status.busy": "2024-04-16T09:44:35.362261Z",
     "iopub.status.idle": "2024-04-16T09:44:35.580369Z",
     "shell.execute_reply": "2024-04-16T09:44:35.579273Z",
     "shell.execute_reply.started": "2024-04-16T09:44:35.362841Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tính toán xem  có bao nhiêu row ứng với mỗi nhãn\n",
    "test_data.groupBy(\"polarity\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f9516",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:44:37.808559Z",
     "iopub.status.busy": "2024-04-16T09:44:37.807258Z",
     "iopub.status.idle": "2024-04-16T09:44:37.919451Z",
     "shell.execute_reply": "2024-04-16T09:44:37.918224Z",
     "shell.execute_reply.started": "2024-04-16T09:44:37.808518Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  show data train\n",
    "training_data.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f5a5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:44:40.960666Z",
     "iopub.status.busy": "2024-04-16T09:44:40.960189Z",
     "iopub.status.idle": "2024-04-16T09:44:41.220975Z",
     "shell.execute_reply": "2024-04-16T09:44:41.219746Z",
     "shell.execute_reply.started": "2024-04-16T09:44:40.960625Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import html\n",
    "schema = \"polarity FLOAT, id LONG, date_time TIMESTAMP, query STRING, user STRING, text STRING\"\n",
    "timestampformat = \"EEE MMM dd HH:mm:ss zzz yyyy\"\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "IN_PATH_RAW = \"/kaggle/input/sparknpl/testdata.csv\"\n",
    "IN_PATH_TRAIN = \"/kaggle/input/sparknpl/training.csv\"\n",
    "#OUT_PATH_CLEAN = \"CLEAN\"\n",
    "\n",
    "spark_reader = spark.read.schema(schema)\n",
    "\n",
    "user_regex = r\"(@\\w{1,15})\"\n",
    "hashtag_regex = \"(#\\w{1,})\"\n",
    "url_regex=r\"((https?|ftp|file):\\/{2,3})+([-\\w+&@#/%=~|$?!:,.]*)|(www.)+([-\\w+&@#/%=~|$?!:,.]*)\"\n",
    "email_regex=r\"[\\w.-]+@[\\w.-]+\\.[a-zA-Z]{1,}\"\n",
    "\n",
    "\n",
    "@f.udf\n",
    "def html_unescape(s: str):\n",
    "    if isinstance(s, str):\n",
    "        return html.unescape(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"original_text\", f.col(\"text\"))\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), url_regex, \"\"))\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), email_regex, \"\"))\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), user_regex, \"\"))\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \"#\", \" \"))\n",
    "        .withColumn(\"text\", html_unescape(f.col(\"text\")))\n",
    "        .filter(\"text != ''\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_train_raw = spark_reader.csv(IN_PATH_RAW, timestampFormat=timestampformat) \n",
    "df_train_clean = clean_data(df_train_raw)\n",
    "df_test_raw = spark_reader.csv(IN_PATH_TEST, timestampFormat=timestampformat) \n",
    "df_test_clean = clean_data(df_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2d291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:44:47.593729Z",
     "iopub.status.busy": "2024-04-16T09:44:47.593267Z",
     "iopub.status.idle": "2024-04-16T09:44:47.665889Z",
     "shell.execute_reply": "2024-04-16T09:44:47.664517Z",
     "shell.execute_reply.started": "2024-04-16T09:44:47.593695Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test_clean_binary = clean_data(df_test_raw_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3d2a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:44:49.440905Z",
     "iopub.status.busy": "2024-04-16T09:44:49.440424Z",
     "iopub.status.idle": "2024-04-16T09:44:50.131925Z",
     "shell.execute_reply": "2024-04-16T09:44:50.130721Z",
     "shell.execute_reply.started": "2024-04-16T09:44:49.440869Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in ra data đã clean\n",
    "df_test_raw_binary = df_test_clean.filter(f.col(\"polarity\")!= 2.0)\n",
    "df_test_raw_binary.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a642ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:44:55.030200Z",
     "iopub.status.busy": "2024-04-16T09:44:55.029798Z",
     "iopub.status.idle": "2024-04-16T09:44:56.299101Z",
     "shell.execute_reply": "2024-04-16T09:44:56.297823Z",
     "shell.execute_reply.started": "2024-04-16T09:44:55.030171Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test data clean not filter\n",
    "df_train_clean.show(10,True)\n",
    "df_test_clean.show(10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e8c383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:44:59.494076Z",
     "iopub.status.busy": "2024-04-16T09:44:59.493665Z",
     "iopub.status.idle": "2024-04-16T09:45:00.183576Z",
     "shell.execute_reply": "2024-04-16T09:45:00.182150Z",
     "shell.execute_reply.started": "2024-04-16T09:44:59.494048Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  đếm độ phổ biến\n",
    "df_test_clean.groupBy(\"polarity\").count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6171220",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Lấy dữ liệu cẩn thiết để Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b6b485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:45:01.945572Z",
     "iopub.status.busy": "2024-04-16T09:45:01.945109Z",
     "iopub.status.idle": "2024-04-16T09:45:02.009800Z",
     "shell.execute_reply": "2024-04-16T09:45:02.008526Z",
     "shell.execute_reply.started": "2024-04-16T09:45:01.945538Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "traindf = (\n",
    "    df_train_clean\n",
    "    # Remove all numbers\n",
    "    .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \"[^a-zA-Z']\", \" \"))\n",
    "    # Remove all double/multiple spaces\n",
    "    .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \" +\", \" \"))\n",
    "    # Remove leading and trailing whitespaces\n",
    "    .withColumn(\"text\", f.trim(f.col(\"text\")))\n",
    "    # Ensure we don't end up with empty rows\n",
    "    .filter(\"text != ''\")\n",
    ")\n",
    "\n",
    "traindata = traindf.select(\"text\", \"polarity\").coalesce(1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09b944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:45:04.490425Z",
     "iopub.status.busy": "2024-04-16T09:45:04.489980Z",
     "iopub.status.idle": "2024-04-16T09:45:04.598165Z",
     "shell.execute_reply": "2024-04-16T09:45:04.597251Z",
     "shell.execute_reply.started": "2024-04-16T09:45:04.490390Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  show data\n",
    "traindata.show(5,truncate =False),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d91b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:45:06.828600Z",
     "iopub.status.busy": "2024-04-16T09:45:06.828063Z",
     "iopub.status.idle": "2024-04-16T09:45:06.890949Z",
     "shell.execute_reply": "2024-04-16T09:45:06.889571Z",
     "shell.execute_reply.started": "2024-04-16T09:45:06.828555Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = (\n",
    "    df_test_clean\n",
    "    # Remove all numbers\n",
    "    .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \"[^a-zA-Z']\", \" \"))\n",
    "    # Remove all double/multiple spaces\n",
    "    .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \" +\", \" \"))\n",
    "    # Remove leading and trailing whitespaces\n",
    "    .withColumn(\"text\", f.trim(f.col(\"text\")))\n",
    "    # Ensure we don't end up with empty rows\n",
    "    .filter(\"text != ''\")\n",
    ")\n",
    "\n",
    "testdata = df_test.select(\"text\", \"polarity\").coalesce(1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379f053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:45:10.120202Z",
     "iopub.status.busy": "2024-04-16T09:45:10.119752Z",
     "iopub.status.idle": "2024-04-16T09:45:10.212472Z",
     "shell.execute_reply": "2024-04-16T09:45:10.211254Z",
     "shell.execute_reply.started": "2024-04-16T09:45:10.120169Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "testdata.show(5,truncate= False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d75bee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:45:12.303188Z",
     "iopub.status.busy": "2024-04-16T09:45:12.302772Z",
     "iopub.status.idle": "2024-04-16T09:45:12.369799Z",
     "shell.execute_reply": "2024-04-16T09:45:12.368902Z",
     "shell.execute_reply.started": "2024-04-16T09:45:12.303157Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test_binary = (\n",
    "    df_test_clean_binary\n",
    "    # Remove all numbers\n",
    "    .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \"[^a-zA-Z']\", \" \"))\n",
    "    # Remove all double/multiple spaces\n",
    "    .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \" +\", \" \"))\n",
    "    # Remove leading and trailing whitespaces\n",
    "    .withColumn(\"text\", f.trim(f.col(\"text\")))\n",
    "    # Ensure we don't end up with empty rows\n",
    "    .filter(\"text != ''\")\n",
    ")\n",
    "\n",
    "testdata_binary = df_test_binary.select(\"text\", \"polarity\").coalesce(1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05906a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:45:15.218772Z",
     "iopub.status.busy": "2024-04-16T09:45:15.218290Z",
     "iopub.status.idle": "2024-04-16T09:45:16.020213Z",
     "shell.execute_reply": "2024-04-16T09:45:16.018633Z",
     "shell.execute_reply.started": "2024-04-16T09:45:15.218736Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489440a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:45:18.473883Z",
     "iopub.status.busy": "2024-04-16T09:45:18.473423Z",
     "iopub.status.idle": "2024-04-16T09:45:18.583014Z",
     "shell.execute_reply": "2024-04-16T09:45:18.581704Z",
     "shell.execute_reply.started": "2024-04-16T09:45:18.473847Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Convert data thành để chuẩn bị test HashingTF\n",
    "import time\n",
    "from pyspark.ml.feature import (\n",
    "    StopWordsRemover,\n",
    "    Tokenizer,\n",
    "    HashingTF,\n",
    "    IDF,\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words1\")\n",
    "stopwords_remover = StopWordsRemover(\n",
    "    inputCol=\"words1\",\n",
    "    outputCol=\"words2\",\n",
    "    stopWords=StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    ")\n",
    "hashing_tf = HashingTF(\n",
    "    inputCol=\"words2\",\n",
    "    outputCol=\"term_frequency\",\n",
    ")\n",
    "idf = IDF(\n",
    "    inputCol=\"term_frequency\",\n",
    "    outputCol=\"features\",\n",
    "    minDocFreq=5,\n",
    ")\n",
    "lr = LogisticRegression(labelCol=\"polarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8d931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:45:21.385541Z",
     "iopub.status.busy": "2024-04-16T09:45:21.384856Z",
     "iopub.status.idle": "2024-04-16T09:46:01.426859Z",
     "shell.execute_reply": "2024-04-16T09:46:01.425536Z",
     "shell.execute_reply.started": "2024-04-16T09:45:21.385498Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tạo mô hình sử dụng pipeline\n",
    "semantic_analysis_pipeline = Pipeline(\n",
    "    stages=[tokenizer, stopwords_remover, hashing_tf, idf, lr]\n",
    ")\n",
    "\n",
    "semantic_analysis_model = semantic_analysis_pipeline.fit(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440504f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:46:26.220898Z",
     "iopub.status.busy": "2024-04-16T09:46:26.220423Z",
     "iopub.status.idle": "2024-04-16T09:46:51.829306Z",
     "shell.execute_reply": "2024-04-16T09:46:51.827905Z",
     "shell.execute_reply.started": "2024-04-16T09:46:26.220866Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tạo mô hình sử dụng pipeline\n",
    "(traindata1, traindata2) = traindata.randomSplit([0.3, 0.7], seed=2020)\n",
    "\n",
    "semantic_analysis_pipeline = Pipeline(\n",
    "    stages=[tokenizer, stopwords_remover, hashing_tf, idf, lr]\n",
    ")\n",
    "semantic_analysis_model = semantic_analysis_pipeline.fit(traindata1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219441e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:47:17.874388Z",
     "iopub.status.busy": "2024-04-16T09:47:17.873944Z",
     "iopub.status.idle": "2024-04-16T09:47:18.300481Z",
     "shell.execute_reply": "2024-04-16T09:47:18.299294Z",
     "shell.execute_reply.started": "2024-04-16T09:47:17.874353Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "semantic_analysis_model.transform(testdata).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2ac62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:47:30.695221Z",
     "iopub.status.busy": "2024-04-16T09:47:30.694749Z",
     "iopub.status.idle": "2024-04-16T09:47:32.185376Z",
     "shell.execute_reply": "2024-04-16T09:47:32.184147Z",
     "shell.execute_reply.started": "2024-04-16T09:47:30.695184Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lưu lại model\n",
    "semantic_analysis_model.save(\"NLP_Model_TW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a69c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T09:47:35.993267Z",
     "iopub.status.busy": "2024-04-16T09:47:35.992865Z",
     "iopub.status.idle": "2024-04-16T09:47:36.356455Z",
     "shell.execute_reply": "2024-04-16T09:47:36.355270Z",
     "shell.execute_reply.started": "2024-04-16T09:47:35.993238Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "semantic_analysis_model.transform(testdata).select(\"polarity\",\"probability\",\"prediction\").show(50,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628debd4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "trainedmodel = PipelineModel.load(\"/kaggle/input/spark-mllib-nlp/NLP_Model\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4755749,
     "sourceId": 8062018,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 93.926786,
   "end_time": "2024-04-16T09:54:59.252046",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-16T09:53:25.325260",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
